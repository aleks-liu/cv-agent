# Why I Want to Join Anthropic

The AI systems we build today will shape what comes next. That's not hyperbole — it's an engineering reality that demands serious people working on security, not as an afterthought, but as a fundamental design constraint. Anthropic is one of the few places that seems to genuinely understand this.

## Alignment with Mission

When Dario Amodei talks publicly about AI risks, he's not doing it for headlines. He's doing it because acknowledging the problem is the first step toward solving it. That honesty resonates with me. Too many organizations treat security as a checkbox or a liability shield. Anthropic's Responsible Scaling Policy and AI Safety Levels framework treat security as a science — something to be measured, tested, and improved systematically.

The idea of "holding light and shade" — acknowledging both AI's potential benefits and its real risks — is how thoughtful security practitioners already think. We don't pretend threats don't exist, and we don't let fear paralyze us. We build systems that work despite adversarial conditions.

## What Excites Me About Anthropic's Approach

Your CISO's philosophy captures something important: "secure by default, private by design — with as little friction as possible." Security that slows everything down eventually gets bypassed. Security that enables people to move fast and stay safe actually gets adopted. Building an AI-powered SOC with Claude demonstrates that Anthropic practices what it preaches — using your own technology to solve real problems in novel ways.

The activation of ASL-3 security standards for Claude Opus 4 shows this isn't theoretical. The security team is working on problems that matter at a scale that matters.

## What I Bring

Eleven years of security engineering have taught me that sustainable security comes from automation, clear architecture, and making the secure path the easy path. I've built security pipelines, hardened Kubernetes clusters, and developed AI-driven tools for code analysis. I understand that protecting frontier AI systems means protecting not just infrastructure, but the research and capabilities that make AI beneficial.

I want to contribute to a team that's racing to the top on safety — not because it's required, but because it's the right thing to do.
