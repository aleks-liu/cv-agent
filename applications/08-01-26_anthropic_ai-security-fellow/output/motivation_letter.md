# Why I Want to Join the Anthropic AI Security Fellowship

After a decade in security, I have seen what happens when systems fail — data breaches, compromised infrastructure, broken trust. But I have also seen something more concerning: the gap between how quickly powerful systems are deployed and how slowly we develop the tools to understand their risks. AI systems represent this challenge at unprecedented scale. The Anthropic AI Security Fellowship offers an opportunity to contribute to solving it.

## Alignment with Anthropic's Mission

Anthropic's commitment to "ignite a race to the top on safety" resonates with how I approach security work. Throughout my career, I have advocated for building security into systems from the start rather than patching it on later. The AI Safety Levels framework represents exactly this thinking — establishing clear security requirements before capabilities outpace our ability to control them. Anthropic treats AI safety as a systematic science, not an afterthought, and I want to contribute to that scientific approach.

The philosophy of "secure by default, private by design — with as little friction as possible" mirrors my own experience. Security that blocks work does not get adopted. Security that enables safe work becomes part of the culture. This pragmatic approach to safety is rare and valuable.

## What Excites Me About This Fellowship

The research produced by previous fellows demonstrates the kind of work I want to do. The SCONE-bench project showed AI agents discovering millions in smart contract vulnerabilities — applying offensive security thinking to AI capability evaluation. The SHADE-Arena work on modular scaffolds for control evaluations directly addresses how to systematically test AI safety boundaries. These projects combine rigorous research methodology with practical security impact.

I am particularly drawn to the intersection of traditional offensive security and AI safety evaluation. How do we red team systems that can adapt? How do we model threats from AI agents? How do we ensure safety properties hold under adversarial conditions? These questions require both security depth and AI understanding.

## What I Bring

My perspective is that of a security practitioner who has spent years breaking systems and building defenses. I have conducted penetration tests, built security automation, and developed AI-driven vulnerability analysis tools. I understand how attackers think because I have been one (professionally). I understand how defenders think because I build their tools.

This practical grounding could complement more theoretical AI safety research. I bring 11 years of asking "what could go wrong?" and 9 years of Python fluency to implement answers quickly. I am accustomed to translating complex security concepts into actionable controls — a skill that seems valuable for turning AI safety research into deployed safeguards.

## Looking Forward

Joining this fellowship would allow me to apply hard-won security expertise to the most consequential systems being built today. I am prepared to commit fully to four months of research, to collaborate with Anthropic's team and fellow researchers, and to produce work that advances the field. The opportunity to contribute to public research on AI safety — work that could benefit the entire ecosystem — is precisely the kind of impact I am seeking in my career.
