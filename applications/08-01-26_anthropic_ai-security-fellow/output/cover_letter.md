Aleksandr Liukov
liukov.aleksandr91@gmail.com | +375 97 888 176
Limassol, Cyprus

January 8, 2026

Dear AI Security Fellowship Selection Committee,

I am applying for the Anthropic AI Security Fellowship with a focused interest in exploring how offensive security techniques can be applied to evaluate and strengthen AI system safety. With 11 years of security experience, Python fluency, and hands-on work building AI-driven security tools, I bring a practitioner's perspective to AI safety research.

My current work bridges traditional security and AI application. At Unlimit, I developed AI-driven tools that examine codebases for business logic vulnerabilities and automate the triage of security scanner findings. I built MCP server integrations for retrieving project-specific security requirements and engineered Python automation that consolidates findings across multiple security scanners. This experience taught me both the power and the limitations of applying AI to security problems — insights I believe are directly relevant to understanding AI system vulnerabilities.

My offensive security foundation — OSCP certification, penetration testing across multiple organizations, and development of automated attack simulations at Positive Technologies — provides the adversarial mindset essential for AI red teaming. I have identified critical vulnerabilities in production systems, including a high-severity flaw in vendor banking software. I approach systems by asking "how can this be broken?" which aligns well with research into AI control and safety evaluation.

The fellowship's focus on producing public research particularly draws me. I see AI safety as the defining security challenge of our generation, and contributing to open research that advances the field would be meaningful work. I am prepared to relocate to Berkeley or London and commit fully to a four-month research engagement.

Sincerely,
Aleksandr Liukov
